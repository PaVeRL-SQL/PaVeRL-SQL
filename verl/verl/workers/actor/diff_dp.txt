2,3d1
< # Copyright 2023-2024 SGLang Team
< # Copyright 2025 ModelBest Inc. and/or its affiliates
21,23c19
< import logging
< import os
< from typing import Tuple
---
> from typing import Iterable, Tuple
29d24
< import verl.utils.torch_functional as verl_F
31,38c26
< from verl.trainer.ppo.core_algos import agg_loss, compute_policy_loss, kl_penalty
< from verl.utils.debug import GPUMemoryLogger
< from verl.utils.device import get_device_name, get_torch_device, is_cuda_available, is_npu_available
< from verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_
< from verl.utils.py_functional import append_to_dict
< from verl.utils.seqlen_balancing import get_reverse_idx, rearrange_micro_batches
< from verl.utils.torch_functional import logprobs_from_logits
< from verl.utils.ulysses import gather_outpus_and_unpad, ulysses_pad_and_slice_inputs
---
> from verl.trainer.ppo import core_algos
39a28,32
> from verl.utils.py_functional import append_to_dict
> from verl.utils.torch_functional import logprobs_from_logits, masked_mean
> from verl.utils.ulysses import ulysses_pad_and_slice_inputs, gather_outpus_and_unpad
> from verl.utils.seqlen_balancing import rearrange_micro_batches, get_reverse_idx
> import verl.utils.torch_functional as verl_F
41,47c34
< if is_cuda_available:
<     from flash_attn.bert_padding import index_first_axis, pad_input, rearrange, unpad_input
< elif is_npu_available:
<     from transformers.integrations.npu_flash_attention import index_first_axis, pad_input, rearrange, unpad_input
< 
< 
< __all__ = ["DataParallelPPOActor"]
---
> from flash_attn.bert_padding import pad_input, unpad_input, rearrange, index_first_axis
49,50c36
< logger = logging.getLogger(__file__)
< logger.setLevel(os.getenv("VERL_LOGGING_LEVEL", "WARN"))
---
> __all__ = ['DataParallelPPOActor']
54c40,46
<     def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):
---
> 
>     def __init__(
>         self,
>         config,
>         actor_module: nn.Module,
>         actor_optimizer: torch.optim.Optimizer = None,
>     ):
59,64c51,52
< 
<         self.use_remove_padding = self.config.get("use_remove_padding", False)
<         print(f"Actor use_remove_padding={self.use_remove_padding}")
<         self.use_fused_kernels = self.config.get("use_fused_kernels", False)
<         print(f"Actor use_fused_kernels={self.use_fused_kernels}")
< 
---
>         self.use_remove_padding = self.config.get('use_remove_padding', False)
>         print(f'Actor use_remove_padding={self.use_remove_padding}')
68,71c56,76
<         self.compute_entropy_from_logits = (
<             torch.compile(verl_F.entropy_from_logits, dynamic=True)
<             if self.config.get("use_torch_compile", True)  #  use torch compile by default
<             else verl_F.entropy_from_logits
---
>         self.compute_entropy_from_logits = torch.compile(verl_F.entropy_from_logits, dynamic=True)
> 
>         if self.config.get("use_dynamic_kl_loss", False):
>             self.train_step = 0
>             self.initial_kl_coef = self.config.kl_loss_coef
>             self.min_kl_coef = 0
>             self.kl_decay_steps = 500
>     
>     def get_kl_coef(self):
>         """计算当前步骤的KL系数，使用余弦退火策略"""
>         if not hasattr(self, "initial_kl_coef"):
>             return self.config.kl_loss_coef
> 
>         if self.train_step >= self.kl_decay_steps:
>             return self.min_kl_coef
> 
>         # 余弦退火计算公式
>         progress = self.train_step / self.kl_decay_steps
>         cosine_decay = 0.5 * (1 + torch.cos(torch.tensor(progress * torch.pi)))
>         return (
>             self.min_kl_coef + (self.initial_kl_coef - self.min_kl_coef) * cosine_decay
73d77
<         self.device_name = get_device_name()
75c79
<     def _forward_micro_batch(self, micro_batch, temperature, calculate_entropy=False) -> Tuple[torch.Tensor, torch.Tensor]:
---
>     def _forward_micro_batch(self, micro_batch, temperature) -> Tuple[torch.Tensor, torch.Tensor]:
77c81
<         Returns:
---
>         Returns: 
81,88c85,87
<         response_length = micro_batch["responses"].size(-1)
<         multi_modal_inputs = {}
<         if "multi_modal_inputs" in micro_batch:
<             for key in micro_batch["multi_modal_inputs"][0].keys():
<                 multi_modal_inputs[key] = torch.cat([inputs[key] for inputs in micro_batch["multi_modal_inputs"]], dim=0)
< 
<         with torch.autocast(device_type=self.device_name, dtype=torch.bfloat16):
<             input_ids = micro_batch["input_ids"]
---
>         response_length = micro_batch['responses'].size(-1)
>         with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
>             input_ids = micro_batch['input_ids']
90,94c89,90
<             attention_mask = micro_batch["attention_mask"]
<             position_ids = micro_batch["position_ids"]
<             entropy = None
<             if position_ids.dim() == 3:  # qwen2vl mrope
<                 position_ids = position_ids.transpose(0, 1)  # (bsz, 3, seqlen) -> (3, bsz, seqlen)
---
>             attention_mask = micro_batch['attention_mask']
>             position_ids = micro_batch['position_ids']
97c93,94
<                 input_ids_rmpad, indices, *_ = unpad_input(input_ids.unsqueeze(-1), attention_mask)  # input_ids_rmpad (total_nnz, ...)
---
>                 input_ids_rmpad, indices, *_ = unpad_input(input_ids.unsqueeze(-1),
>                                                            attention_mask)  # input_ids_rmpad (total_nnz, ...)
101,104c98,99
<                 if position_ids.dim() == 3:
<                     position_ids_rmpad = index_first_axis(rearrange(position_ids, "c b s ... -> (b s) c ..."), indices).transpose(0, 1).unsqueeze(1)  # (3, bsz, seqlen) -> (3, 1, bsz * seqlen)
<                 else:
<                     position_ids_rmpad = index_first_axis(rearrange(position_ids.unsqueeze(-1), "b s ... -> (b s) ..."), indices).transpose(0, 1)
---
>                 position_ids_rmpad = index_first_axis(rearrange(position_ids.unsqueeze(-1), "b s ... -> (b s) ..."),
>                                                       indices).transpose(0, 1)
111,120c106,110
<                     input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad_and_slice_inputs(
<                         input_ids_rmpad,
<                         position_ids_rmpad=position_ids_rmpad,
<                         sp_size=self.ulysses_sequence_parallel_size,
<                     )
<                     input_ids_rmpad_rolled, _, _ = ulysses_pad_and_slice_inputs(
<                         input_ids_rmpad_rolled,
<                         position_ids_rmpad=None,
<                         sp_size=self.ulysses_sequence_parallel_size,
<                     )
---
>                     input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad_and_slice_inputs(input_ids_rmpad, \
>                                                                                                 position_ids_rmpad, \
>                                                                                                 sp_size=self.ulysses_sequence_parallel_size)
>                     input_ids_rmpad_rolled, _, _ = ulysses_pad_and_slice_inputs(input_ids_rmpad_rolled, None,
>                                                                                 self.ulysses_sequence_parallel_size)
125,140c115,119
<                 extra_args = {}
<                 if self.use_fused_kernels:
<                     extra_args["temperature"] = temperature
< 
<                 output = self.actor_module(
<                     input_ids=input_ids_rmpad,
<                     attention_mask=None,
<                     position_ids=position_ids_rmpad,
<                     **multi_modal_inputs,
<                     use_cache=False,
<                     **extra_args,
<                 )  # prevent model thinks we are generating
< 
<                 if self.use_fused_kernels:
<                     log_probs = output.log_probs.squeeze(0)  # (total_nnz,)
<                     entropy_rmpad = output.entropy.squeeze(0)  # (total_nnz,)
---
>                 output = self.actor_module(input_ids=input_ids_rmpad,
>                                            attention_mask=None,
>                                            position_ids=position_ids_rmpad,
>                                            use_cache=False)  # prevent model thinks we are generating
>                 logits_rmpad = output.logits.squeeze(0)  # (total_nnz, vocab_size)
142,144c121,124
<                 else:
<                     logits_rmpad = output.logits.squeeze(0)  # (total_nnz, vocab_size)
<                     logits_rmpad.div_(temperature)
---
>                 logits_rmpad.div_(temperature)
> 
>                 # compute entropy
>                 entropy_rmpad = self.compute_entropy_from_logits(logits_rmpad)  # ((total_nnz / sp) + pad)
146,158c126,127
<                     # if use_sp: ((total_nnz / sp) + pad) ; if not use_sp: (batch, seqlen)
<                     inplace_backward = True
<                     if calculate_entropy:
<                         inplace_backward = False
<                     log_probs = logprobs_from_logits(
<                         logits=logits_rmpad,
<                         labels=input_ids_rmpad_rolled,
<                         inplace_backward=inplace_backward,
<                     )
< 
<                     # compute entropy
<                     if calculate_entropy:
<                         entropy_rmpad = self.compute_entropy_from_logits(logits_rmpad)  # ((total_nnz / sp) + pad)
---
>                 # if use_sp: ((total_nnz / sp) + pad) ; if not use_sp: (batch, seqlen)
>                 log_probs = logprobs_from_logits(logits=logits_rmpad, labels=input_ids_rmpad_rolled)
163,175c132,136
<                     log_probs = gather_outpus_and_unpad(
<                         log_probs,
<                         gather_dim=0,
<                         unpad_dim=0,
<                         padding_size=pad_size,
<                     )
<                     if calculate_entropy:
<                         entropy_rmpad = gather_outpus_and_unpad(
<                             entropy_rmpad,
<                             gather_dim=0,
<                             unpad_dim=0,
<                             padding_size=pad_size,
<                         )
---
>                     log_probs = gather_outpus_and_unpad(log_probs, gather_dim=0, unpad_dim=0, padding_size=pad_size)
>                     entropy_rmpad = gather_outpus_and_unpad(entropy_rmpad,
>                                                             gather_dim=0,
>                                                             unpad_dim=0,
>                                                             padding_size=pad_size)
177,189c138,145
<                 if calculate_entropy:
<                     full_entropy = pad_input(
<                         hidden_states=entropy_rmpad.unsqueeze(-1),
<                         indices=indices,
<                         batch=batch_size,
<                         seqlen=seqlen,
<                     )
<                 full_log_probs = pad_input(
<                     hidden_states=log_probs.unsqueeze(-1),
<                     indices=indices,
<                     batch=batch_size,
<                     seqlen=seqlen,
<                 )
---
>                 full_entropy = pad_input(hidden_states=entropy_rmpad.unsqueeze(-1),
>                                          indices=indices,
>                                          batch=batch_size,
>                                          seqlen=seqlen)
>                 full_log_probs = pad_input(hidden_states=log_probs.unsqueeze(-1),
>                                            indices=indices,
>                                            batch=batch_size,
>                                            seqlen=seqlen)
192,194c148,149
<                 if calculate_entropy:
<                     entropy = full_entropy.squeeze(-1)[:, -response_length - 1 : -1]  # (bsz, response_length)
<                 log_probs = full_log_probs.squeeze(-1)[:, -response_length - 1 : -1]  # (bsz, response_length)
---
>                 entropy = full_entropy.squeeze(-1)[:, -response_length - 1:-1]  # (bsz, response_length)
>                 log_probs = full_log_probs.squeeze(-1)[:, -response_length - 1:-1]  # (bsz, response_length)
197,220c152,160
<                 extra_args = {}
<                 if self.use_fused_kernels:
<                     extra_args["temperature"] = temperature
<                 output = self.actor_module(
<                     input_ids=input_ids,
<                     attention_mask=attention_mask,
<                     position_ids=position_ids,
<                     **multi_modal_inputs,
<                     use_cache=False,
<                     **extra_args,
<                 )  # prevent model thinks we are generating
< 
<                 if self.use_fused_kernels:
<                     log_probs = output.log_probs[:, -response_length - 1 : -1]
<                     entropy = output.entropy[:, -response_length - 1 : -1]  # (bsz, response_length)
< 
<                 else:
<                     logits = output.logits
< 
<                     logits.div_(temperature)
<                     logits = logits[:, -response_length - 1 : -1, :]  # (bsz, response_length, vocab_size)
<                     log_probs = logprobs_from_logits(logits, micro_batch["responses"])
<                     if calculate_entropy:
<                         entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
---
>                 output = self.actor_module(input_ids=input_ids,
>                                            attention_mask=attention_mask,
>                                            position_ids=position_ids,
>                                            use_cache=False)  # prevent model thinks we are generating
>                 logits = output.logits
>                 logits.div_(temperature)
>                 logits = logits[:, -response_length - 1:-1, :]  # (bsz, response_length, vocab_size)
>                 log_probs = logprobs_from_logits(logits, micro_batch['responses'])
>                 entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
229,230d168
<         elif isinstance(self.actor_module, FSDPModule):
<             grad_norm = fsdp2_clip_grad_norm_(self.actor_module.parameters(), max_norm=self.config.grad_clip)
233,239c171
< 
<         # if grad_norm is not finite, skip the update
<         if not torch.isfinite(grad_norm):
<             print(f"WARN: rank {torch.distributed.get_rank()} grad_norm is not finite: {grad_norm}")
<             self.actor_optimizer.zero_grad()
<         else:
<             self.actor_optimizer.step()
---
>         self.actor_optimizer.step()
242,243c174
<     @GPUMemoryLogger(role="dp actor", logger=logger)
<     def compute_log_prob(self, data: DataProto, calculate_entropy=False) -> torch.Tensor:
---
>     def compute_log_prob(self, data: DataProto) -> torch.Tensor:
264,266c195,197
<         micro_batch_size = data.meta_info["micro_batch_size"]
<         temperature = data.meta_info["temperature"]  # temperature must be in the data.meta_info to avoid silent error
<         use_dynamic_bsz = data.meta_info["use_dynamic_bsz"]
---
>         micro_batch_size = data.meta_info['micro_batch_size']
>         temperature = data.meta_info['temperature']  # temperature must be in the data.meta_info to avoid slient error
>         use_dynamic_bsz = data.meta_info['use_dynamic_bsz']
268c199
<         select_keys = ["responses", "input_ids", "attention_mask", "position_ids"]
---
>         select_keys = ['responses', 'input_ids', 'attention_mask', 'position_ids']
270d200
<         has_multi_modal_inputs = "multi_modal_inputs" in data.non_tensor_batch.keys()
272,276c202
<         if has_multi_modal_inputs:
<             num_micro_batches = data.batch.batch_size[0] // micro_batch_size
<             non_tensor_select_keys = ["multi_modal_inputs"]
<             micro_batches = data.select(select_keys, non_tensor_select_keys).chunk(num_micro_batches)
<         elif use_dynamic_bsz:
---
>         if use_dynamic_bsz:
278c204
<             max_token_len = data.meta_info["max_token_len"] * self.ulysses_sequence_parallel_size
---
>             max_token_len = data.meta_info['max_token_len'] * self.ulysses_sequence_parallel_size
284d209
<         entropy_lst = []
286,287d210
<             if isinstance(micro_batch, DataProto):
<                 micro_batch = {**micro_batch.batch, **micro_batch.non_tensor_batch}
289c212
<                 entropy, log_probs = self._forward_micro_batch(micro_batch, temperature=temperature, calculate_entropy=calculate_entropy)
---
>                 _, log_probs = self._forward_micro_batch(micro_batch, temperature=temperature)
291,293d213
<             if calculate_entropy:
<                 entropy_lst.append(entropy)
< 
295,297c215
<         entropys = None
<         if calculate_entropy:
<             entropys = torch.concat(entropy_lst, dim=0)
---
> 
304c222
<         return log_probs, entropys
---
>         return log_probs
306d223
<     @GPUMemoryLogger(role="dp actor", logger=logger)
311,312c228
<         temperature = data.meta_info["temperature"]  # temperature must be in the data.meta_info to avoid silent error
<         multi_turn = data.meta_info.get("multi_turn", False)
---
>         temperature = data.meta_info['temperature']  # temperature must be in the data.meta_info to avoid slient error
314,316c230
<         select_keys = ["responses", "input_ids", "attention_mask", "position_ids", "old_log_probs", "advantages"]
<         if multi_turn:
<             select_keys.append("loss_mask")
---
>         select_keys = ['responses', 'input_ids', 'attention_mask', 'position_ids', 'old_log_probs', 'advantages']
318c232
<             select_keys.append("ref_log_prob")
---
>             select_keys.append('ref_log_prob')
320d233
<         has_multi_modal_inputs = "multi_modal_inputs" in data.non_tensor_batch.keys()
324,329c237
<         if has_multi_modal_inputs:
<             num_mini_batches = data.batch.batch_size[0] // self.config.ppo_mini_batch_size
<             non_tensor_select_keys = ["multi_modal_inputs"]
<             dataloader = data.select(select_keys, non_tensor_select_keys).chunk(num_mini_batches)
<         else:
<             dataloader = batch.split(self.config.ppo_mini_batch_size)
---
>         dataloader = batch.split(self.config.ppo_mini_batch_size)
332,333c240,247
<         for epoch in range(self.config.ppo_epochs):
<             for batch_idx, data in enumerate(dataloader):
---
>         for batch_idx, data in enumerate(dataloader):
>             # split batch into micro_batches
>             mini_batch = data
>             if self.config.use_dynamic_bsz:
>                 max_token_len = self.config.ppo_max_token_len_per_gpu * self.ulysses_sequence_parallel_size
>                 micro_batches, _ = rearrange_micro_batches(batch=mini_batch, max_token_len=max_token_len)
>             else:
>                 self.gradient_accumulation = self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu
335,365c249
<                 mini_batch = data
<                 if has_multi_modal_inputs:
<                     self.gradient_accumulation = self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu
<                     num_micro_batches = mini_batch.batch.batch_size[0] // self.config.ppo_micro_batch_size_per_gpu
<                     micro_batches = data.select(select_keys, non_tensor_select_keys).chunk(num_micro_batches)
<                 elif self.config.use_dynamic_bsz:
<                     max_token_len = self.config.ppo_max_token_len_per_gpu * self.ulysses_sequence_parallel_size
<                     micro_batches, _ = rearrange_micro_batches(batch=mini_batch, max_token_len=max_token_len)
<                 else:
<                     self.gradient_accumulation = self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu
<                     # split batch into micro_batches
<                     micro_batches = mini_batch.split(self.config.ppo_micro_batch_size_per_gpu)
< 
<                 self.actor_optimizer.zero_grad()
< 
<                 for data in micro_batches:
<                     # Support all hardwares
<                     if isinstance(data, DataProto):
<                         data = {**data.batch.to(get_torch_device().current_device()), **data.non_tensor_batch}
<                     else:
<                         data = data.to(get_torch_device().current_device())  # actor device is cpu when using offload
<                     responses = data["responses"]
<                     response_length = responses.size(1)
<                     attention_mask = data["attention_mask"]
<                     if multi_turn:
<                         response_mask = data["loss_mask"][:, -response_length:]
<                     else:
<                         response_mask = attention_mask[:, -response_length:]
< 
<                     old_log_prob = data["old_log_probs"]
<                     advantages = data["advantages"]
---
>                 micro_batches = mini_batch.split(self.config.ppo_micro_batch_size_per_gpu)
367,393c251
<                     clip_ratio = self.config.clip_ratio
<                     clip_ratio_low = self.config.clip_ratio_low if self.config.clip_ratio_low is not None else clip_ratio
<                     clip_ratio_high = self.config.clip_ratio_high if self.config.clip_ratio_high is not None else clip_ratio
<                     clip_ratio_c = self.config.get("clip_ratio_c", 3.0)
<                     entropy_coeff = self.config.entropy_coeff
<                     loss_agg_mode = self.config.loss_agg_mode
< 
<                     # all return: (bsz, response_length)
<                     calculate_entropy = False
<                     if entropy_coeff != 0:
<                         calculate_entropy = True
<                     entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature, calculate_entropy=calculate_entropy)
< 
<                     pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(
<                         old_log_prob=old_log_prob,
<                         log_prob=log_prob,
<                         advantages=advantages,
<                         response_mask=response_mask,
<                         cliprange=clip_ratio,
<                         cliprange_low=clip_ratio_low,
<                         cliprange_high=clip_ratio_high,
<                         clip_ratio_c=clip_ratio_c,
<                         loss_agg_mode=loss_agg_mode,
<                     )
< 
<                     if entropy_coeff != 0:
<                         entropy_loss = agg_loss(loss_mat=entropy, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
---
>             self.actor_optimizer.zero_grad()
395,396c253,295
<                         # compute policy loss
<                         policy_loss = pg_loss - entropy_loss * entropy_coeff
---
>             for data in micro_batches:
>                 data = data.cuda()  # actor device is cpu when using offload
>                 responses = data['responses']
>                 response_length = responses.size(1)
>                 attention_mask = data['attention_mask']
>                 response_mask = attention_mask[:, -response_length:]
>                 old_log_prob = data['old_log_probs']
>                 advantages = data['advantages']
> 
>                 clip_ratio = self.config.clip_ratio
>                 entropy_coeff = self.config.entropy_coeff
> 
>                 # all return: (bsz, response_length)
>                 entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature)
> 
>                 pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(old_log_prob=old_log_prob,
>                                                                               log_prob=log_prob,
>                                                                               advantages=advantages,
>                                                                               eos_mask=response_mask,
>                                                                               cliprange=clip_ratio)
>                 # compute entropy loss from entropy
>                 entropy_loss = verl_F.masked_mean(entropy, response_mask)
> 
>                 # compute policy loss
>                 policy_loss = pg_loss - entropy_loss * entropy_coeff
> 
>                 if self.config.use_kl_loss:
>                     ref_log_prob = data['ref_log_prob']
>                     # compute kl loss
>                     kld = core_algos.kl_penalty(logprob=log_prob,
>                                                 ref_logprob=ref_log_prob,
>                                                 kl_penalty=self.config.kl_loss_type)
>                     kl_loss = masked_mean(kld, response_mask)
> 
>                     if self.config.get("use_dynamic_kl_loss", False):
>                         # self.train_step = global_step
>                         current_kl_coef = self.get_kl_coef()
>                         policy_loss = policy_loss + kl_loss * current_kl_coef
>                         metrics["actor/kl_loss"] = kl_loss.detach().item()
>                         if isinstance(current_kl_coef, torch.Tensor):
>                             metrics["actor/kl_coef"] = current_kl_coef.detach().item()
>                         else:
>                             metrics["actor/kl_coef"] = current_kl_coef
398,405d296
<                         policy_loss = pg_loss
< 
<                     if self.config.use_kl_loss:
<                         ref_log_prob = data["ref_log_prob"]
<                         # compute kl loss
<                         kld = kl_penalty(logprob=log_prob, ref_logprob=ref_log_prob, kl_penalty=self.config.kl_loss_type)
<                         kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
< 
410,423c301,310
<                     if self.config.use_dynamic_bsz:
<                         # relative to the dynamic bsz
<                         loss = policy_loss * (len(data) / self.config.ppo_mini_batch_size)
<                     else:
<                         loss = policy_loss / self.gradient_accumulation
<                     loss.backward()
< 
<                     data = {
<                         "actor/pg_loss": pg_loss.detach().item(),
<                         "actor/pg_clipfrac": pg_clipfrac.detach().item(),
<                         "actor/ppo_kl": ppo_kl.detach().item(),
<                         "actor/pg_clipfrac_lower": pg_clipfrac_lower.detach().item(),
<                     }
<                     append_to_dict(metrics, data)
---
>                     policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef
>                     metrics['actor/kl_loss'] = kl_loss.detach().item()
>                     metrics['actor/kl_coef'] = self.config.kl_loss_coef
> 
>                 if self.config.use_dynamic_bsz:
>                     # relative to the dynamic bsz
>                     loss = policy_loss * (len(data) / self.config.ppo_mini_batch_size)
>                 else:
>                     loss = policy_loss / self.gradient_accumulation
>                 loss.backward()
425,426c312,317
<                 grad_norm = self._optimizer_step()
<                 data = {"actor/grad_norm": grad_norm.detach().item()}
---
>                 data = {
>                     'actor/entropy_loss': entropy_loss.detach().item(),
>                     'actor/pg_loss': pg_loss.detach().item(),
>                     'actor/pg_clipfrac': pg_clipfrac.detach().item(),
>                     'actor/ppo_kl': ppo_kl.detach().item(),
>                 }
427a319,322
> 
>             grad_norm = self._optimizer_step()
>             data = {'actor/grad_norm': grad_norm.detach().item()}
>             append_to_dict(metrics, data)
428a324,325
>         if self.config.get("use_dynamic_kl_loss", False):
>             self.train_step += 1
